{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è PersonaPlex Demo ‚Äî Proof of Concept\n",
    "\n",
    "**Proyecto:** PersonaPlex Callbot (Fase 1)\n",
    "\n",
    "Este notebook prueba PersonaPlex en Google Colab.\n",
    "\n",
    "### ‚ö†Ô∏è Requisitos de GPU:\n",
    "- **T4 (free tier): NO funciona** ‚Äî 15GB VRAM + 12GB RAM insuficiente. El proceso es `Killed` por OOM.\n",
    "- **L4 o superior: Recomendado** ‚Äî Necesitas Colab Pro o cr√©ditos de GPU.\n",
    "- **A100: Ideal** ‚Äî Es el hardware de referencia de NVIDIA.\n",
    "\n",
    "### Antes de empezar:\n",
    "1. ‚úÖ Acepta la licencia del modelo: [nvidia/personaplex-7b-v1](https://huggingface.co/nvidia/personaplex-7b-v1)\n",
    "2. ‚úÖ Ten tu HuggingFace token listo\n",
    "3. ‚úÖ Selecciona Runtime ‚Üí **L4 GPU o superior**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Verificar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\n‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "    if vram_gb < 20:\n",
    "        print(f\"\\n‚ö†Ô∏è  {gpu_name} tiene solo {vram_gb:.0f}GB VRAM.\")\n",
    "        print(\"   PersonaPlex necesita ~17GB. Se usar√° --cpu-offload.\")\n",
    "        print(\"   Si la GPU tiene <16GB VRAM y el sistema <16GB RAM, puede fallar (OOM/Killed).\")\n",
    "        print(\"   T4 (15GB) en Colab Free NO funciona. Usa L4 o superior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Instalar dependencias del sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq && apt-get install -y -qq libopus-dev > /dev/null 2>&1\n",
    "print(\"‚úÖ libopus-dev instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clonar el repositorio e instalar PersonaPlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/personaplex.git\n",
    "%cd personaplex\n",
    "!pip install -q moshi/.\n",
    "!pip install -q accelerate  # Needed for --cpu-offload\n",
    "print(\"\\n‚úÖ PersonaPlex instalado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Autenticaci√≥n con HuggingFace\n",
    "\n",
    "Ingresa tu token de HuggingFace (necesario para descargar los weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Opci√≥n 1: Ingreso manual\n",
    "hf_token = getpass(\"üîë Ingresa tu HuggingFace Token: \")\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# Verificar\n",
    "print(f\"‚úÖ Token configurado ({len(hf_token)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Test Offline ‚Äî Modo Asistente\n",
    "\n",
    "Primer test: usar el audio de prueba incluido con la voz NATF2 (femenina natural).\n",
    "\n",
    "‚ö†Ô∏è Usamos `--cpu-offload` si la GPU tiene <20GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "offload = \"--cpu-offload\" if vram_gb < 20 else \"\"\n",
    "print(f\"GPU VRAM: {vram_gb:.0f}GB ‚Üí {'cpu-offload ON' if offload else 'full GPU'}\")\n",
    "\n",
    "!python -m moshi.offline \\\n",
    "    --voice-prompt \"NATF2.pt\" \\\n",
    "    --input-wav \"assets/test/input_assistant.wav\" \\\n",
    "    --seed 42424242 \\\n",
    "    --output-wav \"output_assistant.wav\" \\\n",
    "    --output-text \"output_assistant.json\" \\\n",
    "    {offload} 2>&1\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"output_assistant.wav\"):\n",
    "    print(f\"\\n‚úÖ Audio generado: output_assistant.wav ({os.path.getsize('output_assistant.wav')} bytes)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå FALL√ì: output_assistant.wav no fue generado.\")\n",
    "    print(\"   Probable causa: OOM. Esta GPU no tiene suficiente memoria.\")\n",
    "    print(\"   Soluci√≥n: Usar una GPU con ‚â•20GB VRAM (L4, A100) o probar localmente con personaplex-mlx.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducir resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "import json\n",
    "\n",
    "print(\"üéß Audio de entrada (lo que 'escuch√≥' PersonaPlex):\")\n",
    "display(Audio(\"assets/test/input_assistant.wav\"))\n",
    "\n",
    "if os.path.exists(\"output_assistant.wav\"):\n",
    "    print(\"\\nüéôÔ∏è Respuesta de PersonaPlex (24kHz):\")\n",
    "    display(Audio(\"output_assistant.wav\", rate=24000))\n",
    "else:\n",
    "    print(\"\\n‚ùå No hay audio de salida ‚Äî la generaci√≥n fall√≥. Ver celda anterior.\")\n",
    "\n",
    "if os.path.exists(\"output_assistant.json\"):\n",
    "    print(\"\\nüìù Transcripci√≥n de la respuesta:\")\n",
    "    with open(\"output_assistant.json\") as f:\n",
    "        transcript = json.load(f)\n",
    "        print(json.dumps(transcript, indent=2))\n",
    "else:\n",
    "    print(\"\\nüìù No hay transcripci√≥n disponible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test Offline ‚Äî Modo Customer Service\n",
    "\n",
    "Segundo test: modo servicio al cliente con voz masculina NATM1 y un prompt de rol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch, os\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "offload = \"--cpu-offload\" if vram_gb < 20 else \"\"\n",
    "\n",
    "!python -m moshi.offline \\\n",
    "    --voice-prompt \"NATM1.pt\" \\\n",
    "    --text-prompt \"$(cat assets/test/prompt_service.txt)\" \\\n",
    "    --input-wav \"assets/test/input_service.wav\" \\\n",
    "    --seed 42424242 \\\n",
    "    --output-wav \"output_service.wav\" \\\n",
    "    --output-text \"output_service.json\" \\\n",
    "    {offload} 2>&1\n",
    "\n",
    "if os.path.exists(\"output_service.wav\"):\n",
    "    print(f\"\\n‚úÖ Audio generado: output_service.wav ({os.path.getsize('output_service.wav')} bytes)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå FALL√ì: output_service.wav no fue generado (OOM probable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "import json\n",
    "\n",
    "print(\"üéß Audio de entrada (cliente):\")\n",
    "display(Audio(\"assets/test/input_service.wav\"))\n",
    "\n",
    "if os.path.exists(\"output_service.wav\"):\n",
    "    print(\"\\nüéôÔ∏è Respuesta de PersonaPlex (agente):\")\n",
    "    display(Audio(\"output_service.wav\", rate=24000))\n",
    "else:\n",
    "    print(\"\\n‚ùå No hay audio de salida.\")\n",
    "\n",
    "if os.path.exists(\"output_service.json\"):\n",
    "    print(\"\\nüìù Transcripci√≥n:\")\n",
    "    with open(\"output_service.json\") as f:\n",
    "        transcript = json.load(f)\n",
    "        print(json.dumps(transcript, indent=2))\n",
    "else:\n",
    "    print(\"\\nüìù No hay transcripci√≥n disponible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Probar diferentes voces\n",
    "\n",
    "Iteramos sobre varias voces para comparar calidad y estilo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, json, os, torch\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "offload_args = [\"--cpu-offload\"] if vram_gb < 20 else []\n",
    "\n",
    "voices_to_test = [\"NATF0\", \"NATF2\", \"NATM0\", \"NATM2\", \"VARF1\", \"VARM1\"]\n",
    "results = {}\n",
    "\n",
    "for voice in voices_to_test:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üé§ Probando voz: {voice}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    out_wav = f\"output_voice_{voice}.wav\"\n",
    "    out_json = f\"output_voice_{voice}.json\"\n",
    "    \n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"moshi.offline\",\n",
    "        \"--voice-prompt\", f\"{voice}.pt\",\n",
    "        \"--input-wav\", \"assets/test/input_assistant.wav\",\n",
    "        \"--seed\", \"42424242\",\n",
    "        \"--output-wav\", out_wav,\n",
    "        \"--output-text\", out_json,\n",
    "    ] + offload_args, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0 and os.path.exists(out_wav):\n",
    "        print(f\"‚úÖ {voice} completado\")\n",
    "        display(Audio(out_wav, rate=24000))\n",
    "        \n",
    "        if os.path.exists(out_json):\n",
    "            with open(out_json) as f:\n",
    "                transcript = json.load(f)\n",
    "                results[voice] = transcript\n",
    "                print(f\"üìù {json.dumps(transcript, indent=2)[:300]}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error con {voice}: {result.stderr[:300] if result.stderr else 'Killed (OOM)'}\")\n",
    "\n",
    "print(f\"\\n\\nüèÅ Voces completadas: {len(results)}/{len(voices_to_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Probar prompt personalizado (Callbot)\n",
    "\n",
    "Simulamos un escenario de callbot tipo OnBotGo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt personalizado estilo callbot\n",
    "custom_prompt = \"\"\"You work for TechSupport Pro which is a technical support service and your name is Sarah. \\\n",
    "Information: You help customers troubleshoot internet connectivity issues. \\\n",
    "Common solutions: restart router (wait 30 seconds), check cable connections, \\\n",
    "run speed test at speedtest.net. If issue persists, schedule technician visit \\\n",
    "(next available: tomorrow 2-4 PM or Thursday 9-11 AM). \\\n",
    "Service costs: Basic plan $29.99/month, Premium $49.99/month with priority support.\"\"\"\n",
    "\n",
    "# Guardar prompt\n",
    "with open(\"custom_prompt.txt\", \"w\") as f:\n",
    "    f.write(custom_prompt)\n",
    "\n",
    "print(\"Prompt guardado ‚úÖ\")\n",
    "print(f\"\\nüìã Prompt:\\n{custom_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch, os\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "offload = \"--cpu-offload\" if vram_gb < 20 else \"\"\n",
    "\n",
    "!python -m moshi.offline \\\n",
    "    --voice-prompt \"NATF2.pt\" \\\n",
    "    --text-prompt \"$(cat custom_prompt.txt)\" \\\n",
    "    --input-wav \"assets/test/input_service.wav\" \\\n",
    "    --seed 42424242 \\\n",
    "    --output-wav \"output_callbot.wav\" \\\n",
    "    --output-text \"output_callbot.json\" \\\n",
    "    {offload} 2>&1\n",
    "\n",
    "if os.path.exists(\"output_callbot.wav\"):\n",
    "    print(f\"\\n‚úÖ Callbot test completado ({os.path.getsize('output_callbot.wav')} bytes)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå FALL√ì (OOM probable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Audio, display\n",
    "import json\n",
    "\n",
    "if os.path.exists(\"output_callbot.wav\"):\n",
    "    print(\"üéôÔ∏è Respuesta del callbot:\")\n",
    "    display(Audio(\"output_callbot.wav\", rate=24000))\n",
    "else:\n",
    "    print(\"‚ùå No hay audio de salida.\")\n",
    "\n",
    "if os.path.exists(\"output_callbot.json\"):\n",
    "    print(\"\\nüìù Transcripci√≥n:\")\n",
    "    with open(\"output_callbot.json\") as f:\n",
    "        transcript = json.load(f)\n",
    "        print(json.dumps(transcript, indent=2))\n",
    "else:\n",
    "    print(\"üìù No hay transcripci√≥n disponible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ M√©tricas de Rendimiento\n",
    "\n",
    "Capturamos tiempos y uso de recursos para evaluar viabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, subprocess, torch\n",
    "\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "offload_args = [\"--cpu-offload\"] if vram_gb < 20 else []\n",
    "\n",
    "print(\"üìä Benchmark: Latencia de generaci√≥n offline\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start = time.time()\n",
    "result = subprocess.run([\n",
    "    \"python\", \"-m\", \"moshi.offline\",\n",
    "    \"--voice-prompt\", \"NATF2.pt\",\n",
    "    \"--input-wav\", \"assets/test/input_assistant.wav\",\n",
    "    \"--seed\", \"12345\",\n",
    "    \"--output-wav\", \"benchmark_output.wav\",\n",
    "    \"--output-text\", \"benchmark_output.json\",\n",
    "] + offload_args, capture_output=True, text=True)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if os.path.exists(\"benchmark_output.wav\"):\n",
    "    import wave\n",
    "    with wave.open(\"assets/test/input_assistant.wav\") as w:\n",
    "        input_duration = w.getnframes() / w.getframerate()\n",
    "\n",
    "    with wave.open(\"benchmark_output.wav\") as w:\n",
    "        output_duration = w.getnframes() / w.getframerate()\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "    print(f\"‚è±Ô∏è  Tiempo total: {elapsed:.1f}s\")\n",
    "    print(f\"üéµ Duraci√≥n input: {input_duration:.1f}s\")\n",
    "    print(f\"üéµ Duraci√≥n output: {output_duration:.1f}s\")\n",
    "    print(f\"‚ö° RTF (Real Time Factor): {elapsed/output_duration:.2f}x\")\n",
    "    print(f\"   (< 1.0 = m√°s r√°pido que tiempo real)\")\n",
    "    !nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"üìã Resumen para PROJECT.md:\")\n",
    "    print(f\"   GPU: {gpu_name} + {'cpu-offload' if offload_args else 'full GPU'}\")\n",
    "    print(f\"   RTF: {elapsed/output_duration:.2f}x\")\n",
    "    print(f\"   Tiempo gen: {elapsed:.1f}s para {output_duration:.1f}s de audio\")\n",
    "else:\n",
    "    print(f\"‚ùå Benchmark fall√≥ ‚Äî no se gener√≥ audio.\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "    print(f\"   VRAM: {vram_gb:.0f}GB\")\n",
    "    print(f\"   Resultado: OOM/Killed despu√©s de {elapsed:.0f}s\")\n",
    "    print(f\"   Conclusi√≥n: Esta GPU no es suficiente para PersonaPlex.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Resultados Fase 1\n",
    "\n",
    "| M√©trica | Valor |\n",
    "|---------|-------|\n",
    "| GPU usada | *(llenar)* |\n",
    "| cpu-offload | S√≠/No |\n",
    "| RTF | *(llenar)* |\n",
    "| Calidad audio | *(evaluar subjetivamente)* |\n",
    "| Mejor voz femenina | *(llenar)* |\n",
    "| Mejor voz masculina | *(llenar)* |\n",
    "| Adherencia al prompt | *(evaluar)* |\n",
    "| Solo ingl√©s | ‚úÖ Confirmado |\n",
    "\n",
    "### ‚ö†Ô∏è Hallazgo: T4 Colab Free NO funciona\n",
    "- T4 (15GB VRAM, ~12GB RAM) ‚Üí Proceso `Killed` por OOM al cargar moshi (16.7GB)\n",
    "- M√≠nimo requerido: GPU con ‚â•20GB VRAM o sistema con ‚â•24GB RAM para cpu-offload\n",
    "- Alternativa local: personaplex-mlx en Apple Silicon (RAM unificada)\n",
    "\n",
    "### Pr√≥ximos pasos:\n",
    "- [ ] Probar en Mac mini con personaplex-mlx (Fase 1, tarea 2)\n",
    "- [ ] Evaluar si RTF < 1.0 es alcanzable en local\n",
    "- [ ] Dise√±ar integraci√≥n con Twilio (Fase 2)"
   ]
  }
 ]
}